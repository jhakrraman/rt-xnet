{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "dataset LOL_v1\n",
      "Not using Automatic Mixed Precision\n",
      "===>Testing using weights:  pretrained_weights/LOL_v1.pth\n",
      "data/LOLv1/Test/input\n",
      "data/LOLv1/Test/target\n",
      "100%|███████████████████████████████████████████| 15/15 [00:03<00:00,  4.55it/s]\n",
      "PSNR: 25.154886 \n",
      "SSIM: 0.845461 \n"
     ]
    }
   ],
   "source": [
    "!python3 Enhancement/test_from_dataset.py --opt Options/RetinexFormer_LOL_v1.yml --weights pretrained_weights/LOL_v1.pth --dataset LOL_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetinexFormer(\n",
      "  (body): Sequential(\n",
      "    (0): RetinexFormer_Single_Stage(\n",
      "      (estimator): Illumination_Estimator(\n",
      "        (conv1): Conv2d(4, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (depth_conv): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=4)\n",
      "        (conv2): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (denoiser): Denoiser(\n",
      "        (embedding): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (encoder_layers): ModuleList(\n",
      "          (0): ModuleList(\n",
      "            (0): IGAB(\n",
      "              (blocks): ModuleList(\n",
      "                (0): ModuleList(\n",
      "                  (0): IG_MSA(\n",
      "                    (to_q): Linear(in_features=24, out_features=24, bias=False)\n",
      "                    (to_k): Linear(in_features=24, out_features=24, bias=False)\n",
      "                    (to_v): Linear(in_features=24, out_features=24, bias=False)\n",
      "                    (proj): Linear(in_features=24, out_features=24, bias=True)\n",
      "                    (pos_emb): Sequential(\n",
      "                      (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "                      (1): GELU()\n",
      "                      (2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (1): PreNorm(\n",
      "                    (fn): FeedForward(\n",
      "                      (net): Sequential(\n",
      "                        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                        (1): GELU()\n",
      "                        (2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "                        (3): GELU()\n",
      "                        (4): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      )\n",
      "                    )\n",
      "                    (norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (2): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          )\n",
      "          (1): ModuleList(\n",
      "            (0): IGAB(\n",
      "              (blocks): ModuleList(\n",
      "                (0): ModuleList(\n",
      "                  (0): IG_MSA(\n",
      "                    (to_q): Linear(in_features=48, out_features=48, bias=False)\n",
      "                    (to_k): Linear(in_features=48, out_features=48, bias=False)\n",
      "                    (to_v): Linear(in_features=48, out_features=48, bias=False)\n",
      "                    (proj): Linear(in_features=48, out_features=48, bias=True)\n",
      "                    (pos_emb): Sequential(\n",
      "                      (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "                      (1): GELU()\n",
      "                      (2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (1): PreNorm(\n",
      "                    (fn): FeedForward(\n",
      "                      (net): Sequential(\n",
      "                        (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                        (1): GELU()\n",
      "                        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "                        (3): GELU()\n",
      "                        (4): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      )\n",
      "                    )\n",
      "                    (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (2): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): IGAB(\n",
      "          (blocks): ModuleList(\n",
      "            (0): ModuleList(\n",
      "              (0): IG_MSA(\n",
      "                (to_q): Linear(in_features=96, out_features=96, bias=False)\n",
      "                (to_k): Linear(in_features=96, out_features=96, bias=False)\n",
      "                (to_v): Linear(in_features=96, out_features=96, bias=False)\n",
      "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (pos_emb): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "                  (1): GELU()\n",
      "                  (2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "                )\n",
      "              )\n",
      "              (1): PreNorm(\n",
      "                (fn): FeedForward(\n",
      "                  (net): Sequential(\n",
      "                    (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                    (1): GELU()\n",
      "                    (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                    (3): GELU()\n",
      "                    (4): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (decoder_layers): ModuleList(\n",
      "          (0): ModuleList(\n",
      "            (0): ConvTranspose2d(96, 48, kernel_size=(2, 2), stride=(2, 2))\n",
      "            (1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (2): IGAB(\n",
      "              (blocks): ModuleList(\n",
      "                (0): ModuleList(\n",
      "                  (0): IG_MSA(\n",
      "                    (to_q): Linear(in_features=48, out_features=48, bias=False)\n",
      "                    (to_k): Linear(in_features=48, out_features=48, bias=False)\n",
      "                    (to_v): Linear(in_features=48, out_features=48, bias=False)\n",
      "                    (proj): Linear(in_features=48, out_features=48, bias=True)\n",
      "                    (pos_emb): Sequential(\n",
      "                      (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "                      (1): GELU()\n",
      "                      (2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (1): PreNorm(\n",
      "                    (fn): FeedForward(\n",
      "                      (net): Sequential(\n",
      "                        (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                        (1): GELU()\n",
      "                        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "                        (3): GELU()\n",
      "                        (4): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      )\n",
      "                    )\n",
      "                    (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): ModuleList(\n",
      "            (0): ConvTranspose2d(48, 24, kernel_size=(2, 2), stride=(2, 2))\n",
      "            (1): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (2): IGAB(\n",
      "              (blocks): ModuleList(\n",
      "                (0): ModuleList(\n",
      "                  (0): IG_MSA(\n",
      "                    (to_q): Linear(in_features=24, out_features=24, bias=False)\n",
      "                    (to_k): Linear(in_features=24, out_features=24, bias=False)\n",
      "                    (to_v): Linear(in_features=24, out_features=24, bias=False)\n",
      "                    (proj): Linear(in_features=24, out_features=24, bias=True)\n",
      "                    (pos_emb): Sequential(\n",
      "                      (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "                      (1): GELU()\n",
      "                      (2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (1): PreNorm(\n",
      "                    (fn): FeedForward(\n",
      "                      (net): Sequential(\n",
      "                        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                        (1): GELU()\n",
      "                        (2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "                        (3): GELU()\n",
      "                        (4): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      )\n",
      "                    )\n",
      "                    (norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mapping): Conv2d(24, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/einops/einops.py:204: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  inferred_length: int = length // known_product\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 81 time(s)\n",
      "Unsupported operator aten::add encountered 17 time(s)\n",
      "Unsupported operator aten::div encountered 70 time(s)\n",
      "Unsupported operator aten::norm encountered 10 time(s)\n",
      "Unsupported operator aten::clamp_min encountered 10 time(s)\n",
      "Unsupported operator aten::expand_as encountered 10 time(s)\n",
      "Unsupported operator aten::softmax encountered 5 time(s)\n",
      "Unsupported operator aten::gelu encountered 15 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "body.0.denoiser.lrelu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMac:4.3341064453125\n",
      "Params:411709\n"
     ]
    }
   ],
   "source": [
    "from basicsr.models.archs.RetinexFormer_arch import RetinexFormer\n",
    "from Enhancement.utils import my_summary\n",
    "my_summary(RetinexFormer(in_channels=3, out_channels=3, n_feat=24, stage=1, num_blocks=[1, 1, 1]), 256, 256, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disable distributed.\n",
      "iter (20250524_184308),psnr\n",
      "2025-05-24 18:43:08,734 INFO: \n",
      "                ____                _       _____  ____\n",
      "               / __ ) ____ _ _____ (_)_____/ ___/ / __ \\\n",
      "              / __  |/ __ `// ___// // ___/\\__ \\ / /_/ /\n",
      "             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/\n",
      "            /_____/ \\__,_//____//_/ \\___//____//_/ |_|\n",
      "     ______                   __   __                 __      __\n",
      "    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /\n",
      "   / / __ / __ \\ / __ \\ / __  /  / /   / / / // ___// //_/  / /\n",
      "  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/\n",
      "  \\____/ \\____/ \\____/ \\____/  /_____/\\____/ \\___//_/|_|  (_)\n",
      "    \n",
      "Version Information: \n",
      "\tBasicSR: 1.2.0+ad5c091\n",
      "\tPyTorch: 1.11.0\n",
      "\tTorchVision: 0.12.0\n",
      "2025-05-24 18:43:08,734 INFO: \n",
      "  name: RetinexFormer_LOL_v1\n",
      "  model_type: ImageCleanModel\n",
      "  scale: 1\n",
      "  num_gpu: 1\n",
      "  manual_seed: 100\n",
      "  datasets:[\n",
      "    train:[\n",
      "      name: TrainSet\n",
      "      type: Dataset_PairedImage\n",
      "      dataroot_gt: data/LOLv1/Train/target\n",
      "      dataroot_lq: data/LOLv1/Train/input\n",
      "      geometric_augs: True\n",
      "      filename_tmpl: {}\n",
      "      io_backend:[\n",
      "        type: disk\n",
      "      ]\n",
      "      use_shuffle: True\n",
      "      num_worker_per_gpu: 8\n",
      "      batch_size_per_gpu: 8\n",
      "      mini_batch_sizes: [8]\n",
      "      iters: [300000]\n",
      "      gt_size: 128\n",
      "      gt_sizes: [128]\n",
      "      dataset_enlarge_ratio: 1\n",
      "      prefetch_mode: None\n",
      "      phase: train\n",
      "      scale: 1\n",
      "    ]\n",
      "    val:[\n",
      "      name: ValSet\n",
      "      type: Dataset_PairedImage\n",
      "      dataroot_gt: data/LOLv1/Test/target\n",
      "      dataroot_lq: data/LOLv1/Test/input\n",
      "      io_backend:[\n",
      "        type: disk\n",
      "      ]\n",
      "      phase: val\n",
      "      scale: 1\n",
      "    ]\n",
      "  ]\n",
      "  network_g:[\n",
      "    type: RetinexFormer\n",
      "    in_channels: 3\n",
      "    out_channels: 3\n",
      "    n_feat: 40\n",
      "    stage: 1\n",
      "    num_blocks: [1, 2, 2]\n",
      "  ]\n",
      "  path:[\n",
      "    pretrain_network_g: None\n",
      "    strict_load_g: True\n",
      "    resume_state: None\n",
      "    root: /mnt/data/Raman/Retinexformer\n",
      "    experiments_root: /mnt/data/Raman/Retinexformer/experiments/RetinexFormer_LOL_v1\n",
      "    models: /mnt/data/Raman/Retinexformer/experiments/RetinexFormer_LOL_v1/models\n",
      "    training_states: /mnt/data/Raman/Retinexformer/experiments/RetinexFormer_LOL_v1/training_states\n",
      "    log: /mnt/data/Raman/Retinexformer/experiments/RetinexFormer_LOL_v1\n",
      "    visualization: /mnt/data/Raman/Retinexformer/experiments/RetinexFormer_LOL_v1/visualization\n",
      "  ]\n",
      "  train:[\n",
      "    total_iter: 20000\n",
      "    warmup_iter: -1\n",
      "    use_grad_clip: True\n",
      "    scheduler:[\n",
      "      type: CosineAnnealingRestartCyclicLR\n",
      "      periods: [46000, 104000]\n",
      "      restart_weights: [1, 1]\n",
      "      eta_mins: [0.0003, 1e-06]\n",
      "    ]\n",
      "    mixing_augs:[\n",
      "      mixup: True\n",
      "      mixup_beta: 1.2\n",
      "      use_identity: True\n",
      "    ]\n",
      "    optim_g:[\n",
      "      type: Adam\n",
      "      lr: 0.0002\n",
      "      betas: [0.9, 0.999]\n",
      "    ]\n",
      "    pixel_opt:[\n",
      "      type: L1Loss\n",
      "      loss_weight: 1\n",
      "      reduction: mean\n",
      "    ]\n",
      "  ]\n",
      "  val:[\n",
      "    window_size: 4\n",
      "    val_freq: 1000.0\n",
      "    save_img: False\n",
      "    rgb2bgr: True\n",
      "    use_image: False\n",
      "    max_minibatch: 8\n",
      "    metrics:[\n",
      "      psnr:[\n",
      "        type: calculate_psnr\n",
      "        crop_border: 0\n",
      "        test_y_channel: False\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "  logger:[\n",
      "    print_freq: 500\n",
      "    save_checkpoint_freq: 1000.0\n",
      "    use_tb_logger: True\n",
      "    wandb:[\n",
      "      project: low_light\n",
      "      resume_id: None\n",
      "    ]\n",
      "  ]\n",
      "  dist_params:[\n",
      "    backend: nccl\n",
      "    port: 29500\n",
      "  ]\n",
      "  is_train: True\n",
      "  dist: False\n",
      "  rank: 0\n",
      "  world_size: 1\n",
      "  rename_flag: False\n",
      "\n",
      "2025-05-24 18:43:08,882 INFO: Dataset Dataset_PairedImage - TrainSet is created.\n",
      "2025-05-24 18:43:08,882 INFO: Training statistics:\n",
      "\tNumber of train images: 485\n",
      "\tDataset enlarge ratio: 1\n",
      "\tBatch size per gpu: 8\n",
      "\tWorld size (gpu number): 1\n",
      "\tRequire iter number per epoch: 61\n",
      "\tTotal epochs: 328; iters: 20000.\n",
      "2025-05-24 18:43:08,882 INFO: Dataset Dataset_PairedImage - ValSet is created.\n",
      "2025-05-24 18:43:08,883 INFO: Number of val images/folders in ValSet: 15\n",
      "Not using Automatic Mixed Precision\n",
      "2025-05-24 18:43:11,108 INFO: Model [ImageCleanModel] is created.\n",
      "2025-05-24 18:43:11,374 INFO: Start training from epoch: 0, iter: 0\n",
      "2025-05-24 18:43:11,962 INFO: \n",
      " Updating Patch_Size to 128 and Batch_Size to 16 \n",
      "\n",
      "2025-05-24 18:43:54,475 INFO: [Retin..][epoch:  8, iter:     500, lr:(2.000e-04,)] [eta: 0:28:07, time (data): 0.077 (0.001)] l_pix: 1.2418e-01 \n",
      "2025-05-24 18:44:35,987 INFO: [Retin..][epoch: 16, iter:   1,000, lr:(2.001e-04,)] [eta: 0:26:51, time (data): 0.077 (0.001)] l_pix: 1.4023e-01 \n",
      "2025-05-24 18:44:35,987 INFO: Saving models and training states.\n",
      "2025-05-24 18:44:37,379 INFO: Validation ValSet,\t\t # psnr: 20.1721\n",
      "1000,20.17211543518733\n",
      "2025-05-24 18:45:18,701 INFO: [Retin..][epoch: 24, iter:   1,500, lr:(2.003e-04,)] [eta: 0:26:12, time (data): 0.076 (0.001)] l_pix: 9.4854e-02 \n",
      "2025-05-24 18:46:00,499 INFO: [Retin..][epoch: 33, iter:   2,000, lr:(2.005e-04,)] [eta: 0:25:23, time (data): 0.077 (0.001)] l_pix: 7.1680e-02 \n",
      "2025-05-24 18:46:00,500 INFO: Saving models and training states.\n",
      "2025-05-24 18:46:01,717 INFO: Validation ValSet,\t\t # psnr: 20.5406\n",
      "2000,20.540589487575396\n",
      "2025-05-24 18:46:43,173 INFO: [Retin..][epoch: 41, iter:   2,500, lr:(2.007e-04,)] [eta: 0:24:43, time (data): 0.077 (0.001)] l_pix: 1.2819e-01 \n",
      "2025-05-24 18:47:24,731 INFO: [Retin..][epoch: 49, iter:   3,000, lr:(2.010e-04,)] [eta: 0:23:56, time (data): 0.077 (0.001)] l_pix: 8.8219e-02 \n",
      "2025-05-24 18:47:24,731 INFO: Saving models and training states.\n",
      "2025-05-24 18:47:26,030 INFO: Validation ValSet,\t\t # psnr: 20.7299\n",
      "3000,20.72987280309354\n",
      "2025-05-24 18:48:07,785 INFO: [Retin..][epoch: 58, iter:   3,500, lr:(2.014e-04,)] [eta: 0:23:18, time (data): 0.077 (0.001)] l_pix: 9.7722e-02 \n",
      "2025-05-24 18:48:49,199 INFO: [Retin..][epoch: 66, iter:   4,000, lr:(2.019e-04,)] [eta: 0:22:31, time (data): 0.076 (0.001)] l_pix: 8.2748e-02 \n",
      "2025-05-24 18:48:49,199 INFO: Saving models and training states.\n",
      "2025-05-24 18:48:50,494 INFO: Validation ValSet,\t\t # psnr: 19.9485\n",
      "4000,19.948512140772245\n",
      "2025-05-24 18:49:31,855 INFO: [Retin..][epoch: 74, iter:   4,500, lr:(2.023e-04,)] [eta: 0:21:51, time (data): 0.077 (0.001)] l_pix: 5.9886e-02 \n",
      "2025-05-24 18:50:13,652 INFO: [Retin..][epoch: 83, iter:   5,000, lr:(2.029e-04,)] [eta: 0:21:07, time (data): 0.077 (0.001)] l_pix: 9.2201e-02 \n",
      "2025-05-24 18:50:13,652 INFO: Saving models and training states.\n",
      "2025-05-24 18:50:14,919 INFO: Validation ValSet,\t\t # psnr: 20.5263\n",
      "5000,20.52628265748739\n",
      "2025-05-24 18:50:56,468 INFO: [Retin..][epoch: 91, iter:   5,500, lr:(2.035e-04,)] [eta: 0:20:26, time (data): 0.077 (0.001)] l_pix: 1.0727e-01 \n",
      "2025-05-24 18:51:38,039 INFO: [Retin..][epoch: 99, iter:   6,000, lr:(2.041e-04,)] [eta: 0:19:42, time (data): 0.078 (0.001)] l_pix: 7.0440e-02 \n",
      "2025-05-24 18:51:38,039 INFO: Saving models and training states.\n",
      "2025-05-24 18:51:39,330 INFO: Validation ValSet,\t\t # psnr: 20.5530\n",
      "6000,20.55302899214936\n",
      "2025-05-24 18:52:20,967 INFO: [Retin..][epoch:108, iter:   6,500, lr:(2.048e-04,)] [eta: 0:19:01, time (data): 0.077 (0.001)] l_pix: 9.3956e-02 \n",
      "2025-05-24 18:53:02,489 INFO: [Retin..][epoch:116, iter:   7,000, lr:(2.056e-04,)] [eta: 0:18:18, time (data): 0.077 (0.001)] l_pix: 7.6071e-02 \n",
      "2025-05-24 18:53:02,490 INFO: Saving models and training states.\n",
      "2025-05-24 18:53:03,777 INFO: Validation ValSet,\t\t # psnr: 20.0674\n",
      "7000,20.067438487299103\n",
      "2025-05-24 18:53:45,241 INFO: [Retin..][epoch:124, iter:   7,500, lr:(2.064e-04,)] [eta: 0:17:36, time (data): 0.077 (0.001)] l_pix: 9.2103e-02 \n",
      "2025-05-24 18:54:27,035 INFO: [Retin..][epoch:133, iter:   8,000, lr:(2.073e-04,)] [eta: 0:16:53, time (data): 0.076 (0.001)] l_pix: 8.2071e-02 \n",
      "2025-05-24 18:54:27,035 INFO: Saving models and training states.\n",
      "2025-05-24 18:54:28,347 INFO: Validation ValSet,\t\t # psnr: 21.2014\n",
      "8000,21.201409271978072\n",
      "2025-05-24 18:55:10,025 INFO: [Retin..][epoch:141, iter:   8,500, lr:(2.082e-04,)] [eta: 0:16:12, time (data): 0.076 (0.001)] l_pix: 1.3187e-01 \n",
      "2025-05-24 18:55:51,435 INFO: [Retin..][epoch:149, iter:   9,000, lr:(2.091e-04,)] [eta: 0:15:29, time (data): 0.076 (0.001)] l_pix: 9.6841e-02 \n",
      "2025-05-24 18:55:51,436 INFO: Saving models and training states.\n",
      "2025-05-24 18:55:52,725 INFO: Validation ValSet,\t\t # psnr: 20.5420\n",
      "9000,20.54203772947473\n",
      "2025-05-24 18:56:34,488 INFO: [Retin..][epoch:158, iter:   9,500, lr:(2.102e-04,)] [eta: 0:14:47, time (data): 0.077 (0.001)] l_pix: 8.6081e-02 \n",
      "2025-05-24 18:57:15,842 INFO: [Retin..][epoch:166, iter:  10,000, lr:(2.112e-04,)] [eta: 0:14:04, time (data): 0.076 (0.001)] l_pix: 7.8643e-02 \n",
      "2025-05-24 18:57:15,842 INFO: Saving models and training states.\n",
      "2025-05-24 18:57:17,109 INFO: Validation ValSet,\t\t # psnr: 20.5328\n",
      "10000,20.53275605838576\n",
      "2025-05-24 18:57:58,601 INFO: [Retin..][epoch:174, iter:  10,500, lr:(2.123e-04,)] [eta: 0:13:22, time (data): 0.077 (0.001)] l_pix: 7.3117e-02 \n",
      "2025-05-24 18:58:40,386 INFO: [Retin..][epoch:183, iter:  11,000, lr:(2.135e-04,)] [eta: 0:12:40, time (data): 0.077 (0.001)] l_pix: 7.6334e-02 \n",
      "2025-05-24 18:58:40,386 INFO: Saving models and training states.\n",
      "2025-05-24 18:58:41,672 INFO: Validation ValSet,\t\t # psnr: 21.2347\n",
      "11000,21.234682316146454\n",
      "2025-05-24 18:59:23,754 INFO: [Retin..][epoch:191, iter:  11,500, lr:(2.146e-04,)] [eta: 0:11:58, time (data): 0.077 (0.001)] l_pix: 6.4069e-02 \n",
      "2025-05-24 19:00:05,350 INFO: [Retin..][epoch:199, iter:  12,000, lr:(2.159e-04,)] [eta: 0:11:16, time (data): 0.076 (0.001)] l_pix: 5.9336e-02 \n",
      "2025-05-24 19:00:05,351 INFO: Saving models and training states.\n",
      "2025-05-24 19:00:06,771 INFO: Validation ValSet,\t\t # psnr: 20.8672\n",
      "12000,20.867202332626015\n",
      "2025-05-24 19:00:48,595 INFO: [Retin..][epoch:208, iter:  12,500, lr:(2.171e-04,)] [eta: 0:10:34, time (data): 0.077 (0.001)] l_pix: 4.9335e-02 \n",
      "2025-05-24 19:01:30,160 INFO: [Retin..][epoch:216, iter:  13,000, lr:(2.184e-04,)] [eta: 0:09:51, time (data): 0.078 (0.001)] l_pix: 1.4029e-01 \n",
      "2025-05-24 19:01:30,161 INFO: Saving models and training states.\n",
      "2025-05-24 19:01:31,583 INFO: Validation ValSet,\t\t # psnr: 21.1552\n",
      "13000,21.155157859092178\n",
      "2025-05-24 19:02:13,249 INFO: [Retin..][epoch:224, iter:  13,500, lr:(2.198e-04,)] [eta: 0:09:09, time (data): 0.077 (0.001)] l_pix: 1.1097e-01 \n",
      "2025-05-24 19:02:55,363 INFO: [Retin..][epoch:233, iter:  14,000, lr:(2.212e-04,)] [eta: 0:08:27, time (data): 0.076 (0.001)] l_pix: 9.8146e-02 \n",
      "2025-05-24 19:02:55,363 INFO: Saving models and training states.\n",
      "2025-05-24 19:02:56,792 INFO: Validation ValSet,\t\t # psnr: 21.3781\n",
      "14000,21.378143730345016\n",
      "2025-05-24 19:03:38,405 INFO: [Retin..][epoch:241, iter:  14,500, lr:(2.226e-04,)] [eta: 0:07:45, time (data): 0.077 (0.001)] l_pix: 7.4941e-02 \n",
      "2025-05-24 19:04:19,878 INFO: [Retin..][epoch:249, iter:  15,000, lr:(2.240e-04,)] [eta: 0:07:02, time (data): 0.076 (0.001)] l_pix: 9.7400e-02 \n",
      "2025-05-24 19:04:19,879 INFO: Saving models and training states.\n",
      "2025-05-24 19:04:21,275 INFO: Validation ValSet,\t\t # psnr: 21.1477\n",
      "15000,21.147719412876953\n",
      "2025-05-24 19:05:03,323 INFO: [Retin..][epoch:258, iter:  15,500, lr:(2.255e-04,)] [eta: 0:06:20, time (data): 0.076 (0.001)] l_pix: 8.8146e-02 \n",
      "2025-05-24 19:05:45,038 INFO: [Retin..][epoch:266, iter:  16,000, lr:(2.270e-04,)] [eta: 0:05:38, time (data): 0.077 (0.001)] l_pix: 9.5665e-02 \n",
      "2025-05-24 19:05:45,038 INFO: Saving models and training states.\n",
      "2025-05-24 19:05:46,329 INFO: Validation ValSet,\t\t # psnr: 20.7878\n",
      "16000,20.787783071207397\n",
      "2025-05-24 19:06:27,740 INFO: [Retin..][epoch:274, iter:  16,500, lr:(2.285e-04,)] [eta: 0:04:56, time (data): 0.076 (0.001)] l_pix: 8.4170e-02 \n",
      "2025-05-24 19:07:09,907 INFO: [Retin..][epoch:283, iter:  17,000, lr:(2.301e-04,)] [eta: 0:04:13, time (data): 0.077 (0.001)] l_pix: 6.2020e-02 \n",
      "2025-05-24 19:07:09,908 INFO: Saving models and training states.\n",
      "2025-05-24 19:07:11,375 INFO: Validation ValSet,\t\t # psnr: 21.2070\n",
      "17000,21.207004120103377\n",
      "2025-05-24 19:07:52,877 INFO: [Retin..][epoch:291, iter:  17,500, lr:(2.317e-04,)] [eta: 0:03:31, time (data): 0.076 (0.001)] l_pix: 7.2230e-02 \n",
      "2025-05-24 19:08:34,473 INFO: [Retin..][epoch:299, iter:  18,000, lr:(2.333e-04,)] [eta: 0:02:49, time (data): 0.076 (0.001)] l_pix: 1.0232e-01 \n",
      "2025-05-24 19:08:34,473 INFO: Saving models and training states.\n",
      "2025-05-24 19:08:35,895 INFO: Validation ValSet,\t\t # psnr: 21.2998\n",
      "18000,21.29981587660787\n",
      "2025-05-24 19:09:17,983 INFO: [Retin..][epoch:308, iter:  18,500, lr:(2.349e-04,)] [eta: 0:02:06, time (data): 0.077 (0.001)] l_pix: 9.9323e-02 \n",
      "2025-05-24 19:09:59,505 INFO: [Retin..][epoch:316, iter:  19,000, lr:(2.365e-04,)] [eta: 0:01:24, time (data): 0.076 (0.001)] l_pix: 5.0733e-02 \n",
      "2025-05-24 19:09:59,505 INFO: Saving models and training states.\n",
      "2025-05-24 19:10:00,958 INFO: Validation ValSet,\t\t # psnr: 21.7349\n",
      "19000,21.734869529235475\n",
      "2025-05-24 19:10:42,496 INFO: [Retin..][epoch:324, iter:  19,500, lr:(2.382e-04,)] [eta: 0:00:42, time (data): 0.076 (0.001)] l_pix: 7.9660e-02 \n",
      "2025-05-24 19:11:24,350 INFO: [Retin..][epoch:333, iter:  20,000, lr:(2.398e-04,)] [eta: 0:00:00, time (data): 0.077 (0.001)] l_pix: 8.0424e-02 \n",
      "2025-05-24 19:11:24,350 INFO: Saving models and training states.\n",
      "2025-05-24 19:11:25,788 INFO: Validation ValSet,\t\t # psnr: 21.3459\n",
      "20000,21.345854479914873\n",
      "2025-05-24 19:11:25,790 INFO: End of training. Time consumed: 0:28:14\n",
      "2025-05-24 19:11:25,790 INFO: Save the latest model.\n",
      "2025-05-24 19:11:27,094 INFO: Validation ValSet,\t\t # psnr: 21.3448\n"
     ]
    }
   ],
   "source": [
    "!python3 basicsr/train.py --opt Options/RetinexFormer_LOL_v1.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disable distributed.\n",
      "iter (20250607_104700),psnr\n",
      "2025-06-07 10:47:00,342 INFO: \n",
      "                ____                _       _____  ____\n",
      "               / __ ) ____ _ _____ (_)_____/ ___/ / __ \\\n",
      "              / __  |/ __ `// ___// // ___/\\__ \\ / /_/ /\n",
      "             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/\n",
      "            /_____/ \\__,_//____//_/ \\___//____//_/ |_|\n",
      "     ______                   __   __                 __      __\n",
      "    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /\n",
      "   / / __ / __ \\ / __ \\ / __  /  / /   / / / // ___// //_/  / /\n",
      "  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/\n",
      "  \\____/ \\____/ \\____/ \\____/  /_____/\\____/ \\___//_/|_|  (_)\n",
      "    \n",
      "Version Information: \n",
      "\tBasicSR: 1.2.0+ad5c091\n",
      "\tPyTorch: 1.11.0\n",
      "\tTorchVision: 0.12.0\n",
      "2025-06-07 10:47:00,342 INFO: \n",
      "  name: RTxNet_LLVIP\n",
      "  model_type: ImageCleanModel\n",
      "  scale: 1\n",
      "  num_gpu: 1\n",
      "  manual_seed: 100\n",
      "  datasets:[\n",
      "    train:[\n",
      "      name: TrainSet\n",
      "      type: Dataset_PairedImage\n",
      "      dataroot_gt: data/LLVIP/train/target\n",
      "      dataroot_lq: data/LLVIP/train/input\n",
      "      dataroot_th: data/LLVIP/train/thermal\n",
      "      geometric_augs: True\n",
      "      filename_tmpl: {}\n",
      "      io_backend:[\n",
      "        type: disk\n",
      "      ]\n",
      "      use_shuffle: True\n",
      "      num_worker_per_gpu: 8\n",
      "      batch_size_per_gpu: 8\n",
      "      mini_batch_sizes: [8]\n",
      "      iters: [300000]\n",
      "      gt_size: 128\n",
      "      gt_sizes: [128]\n",
      "      dataset_enlarge_ratio: 1\n",
      "      prefetch_mode: None\n",
      "      phase: train\n",
      "      scale: 1\n",
      "    ]\n",
      "    val:[\n",
      "      name: ValSet\n",
      "      type: Dataset_PairedImage\n",
      "      dataroot_gt: data/LLVIP/test/target\n",
      "      dataroot_lq: data/LLVIP/test/input\n",
      "      dataroot_th: data/LLVIP/test/thermal\n",
      "      io_backend:[\n",
      "        type: disk\n",
      "      ]\n",
      "      phase: val\n",
      "      scale: 1\n",
      "    ]\n",
      "  ]\n",
      "  network_g:[\n",
      "    type: RTxNet\n",
      "    in_channels: 3\n",
      "    out_channels: 3\n",
      "    n_feat: 40\n",
      "    stage: 1\n",
      "    num_blocks: [1, 2, 2]\n",
      "  ]\n",
      "  path:[\n",
      "    pretrain_network_g: None\n",
      "    strict_load_g: True\n",
      "    resume_state: None\n",
      "    root: /mnt/data/Raman/Retinexformer\n",
      "    experiments_root: /mnt/data/Raman/Retinexformer/experiments/RTxNet_LLVIP\n",
      "    models: /mnt/data/Raman/Retinexformer/experiments/RTxNet_LLVIP/models\n",
      "    training_states: /mnt/data/Raman/Retinexformer/experiments/RTxNet_LLVIP/training_states\n",
      "    log: /mnt/data/Raman/Retinexformer/experiments/RTxNet_LLVIP\n",
      "    visualization: /mnt/data/Raman/Retinexformer/experiments/RTxNet_LLVIP/visualization\n",
      "  ]\n",
      "  train:[\n",
      "    total_iter: 20000\n",
      "    warmup_iter: -1\n",
      "    use_grad_clip: True\n",
      "    scheduler:[\n",
      "      type: CosineAnnealingRestartCyclicLR\n",
      "      periods: [46000, 104000]\n",
      "      restart_weights: [1, 1]\n",
      "      eta_mins: [0.0003, 1e-06]\n",
      "    ]\n",
      "    mixing_augs:[\n",
      "      mixup: True\n",
      "      mixup_beta: 1.2\n",
      "      use_identity: True\n",
      "    ]\n",
      "    optim_g:[\n",
      "      type: Adam\n",
      "      lr: 0.0002\n",
      "      betas: [0.9, 0.999]\n",
      "    ]\n",
      "    pixel_opt:[\n",
      "      type: L1Loss\n",
      "      loss_weight: 1\n",
      "      reduction: mean\n",
      "    ]\n",
      "  ]\n",
      "  val:[\n",
      "    window_size: 4\n",
      "    val_freq: 1000.0\n",
      "    save_img: False\n",
      "    rgb2bgr: True\n",
      "    use_image: False\n",
      "    max_minibatch: 8\n",
      "    metrics:[\n",
      "      psnr:[\n",
      "        type: calculate_psnr\n",
      "        crop_border: 0\n",
      "        test_y_channel: False\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "  logger:[\n",
      "    print_freq: 500\n",
      "    save_checkpoint_freq: 1000.0\n",
      "    use_tb_logger: True\n",
      "    wandb:[\n",
      "      project: low_light\n",
      "      resume_id: None\n",
      "    ]\n",
      "  ]\n",
      "  dist_params:[\n",
      "    backend: nccl\n",
      "    port: 29500\n",
      "  ]\n",
      "  is_train: True\n",
      "  dist: False\n",
      "  rank: 0\n",
      "  world_size: 1\n",
      "  rename_flag: False\n",
      "\n",
      "2025-06-07 10:47:00,578 INFO: Dataset Dataset_PairedImage - TrainSet is created.\n",
      "2025-06-07 10:47:00,578 INFO: Training statistics:\n",
      "\tNumber of train images: 481\n",
      "\tDataset enlarge ratio: 1\n",
      "\tBatch size per gpu: 8\n",
      "\tWorld size (gpu number): 1\n",
      "\tRequire iter number per epoch: 61\n",
      "\tTotal epochs: 328; iters: 20000.\n",
      "2025-06-07 10:47:00,580 INFO: Dataset Dataset_PairedImage - ValSet is created.\n",
      "2025-06-07 10:47:00,580 INFO: Number of val images/folders in ValSet: 70\n",
      "Not using Automatic Mixed Precision\n",
      "2025-06-07 10:47:02,608 INFO: Model [ImageCleanModel] is created.\n",
      "2025-06-07 10:47:02,774 INFO: Start training from epoch: 0, iter: 0\n",
      "2025-06-07 10:47:04,752 INFO: \n",
      " Updating Patch_Size to 128 and Batch_Size to 16 \n",
      "\n",
      "2025-06-07 10:51:08,715 INFO: [RTxNe..][epoch:  8, iter:     500, lr:(2.000e-04,)] [eta: 2:39:38, time (data): 0.461 (0.001)] l_pix: 1.5453e-02 \n",
      "2025-06-07 10:55:10,057 INFO: [RTxNe..][epoch: 16, iter:   1,000, lr:(2.001e-04,)] [eta: 2:34:11, time (data): 0.494 (0.001)] l_pix: 1.7734e-02 \n",
      "2025-06-07 10:55:10,057 INFO: Saving models and training states.\n",
      "2025-06-07 11:01:25,300 INFO: Validation ValSet,\t\t # psnr: 33.7191\n",
      "1000,33.719090572677544\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"basicsr/train.py\", line 363, in <module>\n",
      "    main()\n",
      "  File \"basicsr/train.py\", line 300, in main\n",
      "    model.optimize_parameters(current_iter)\n",
      "  File \"/mnt/data/Raman/Retinexformer/basicsr/models/image_restoration_model.py\", line 223, in optimize_parameters\n",
      "    torch.nn.utils.clip_grad_norm_(self.net_g.parameters(), 0.01)\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\", line 55, in clip_grad_norm_\n",
      "    p.grad.detach().mul_(clip_coef_clamped.to(p.grad.device))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 basicsr/train.py --opt Options/RTxNet_LLVIP.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter Count Table:\n",
      "| name                        | #elements or shape   |\n",
      "|:----------------------------|:---------------------|\n",
      "| model                       | 0.7M                 |\n",
      "|  body                       |  0.7M                |\n",
      "|   body.0                    |   0.7M               |\n",
      "|    body.0.estimator_rgb     |    6.7K              |\n",
      "|    body.0.estimator_thermal |    6.7K              |\n",
      "|    body.0.cross_attention   |    14.1K             |\n",
      "|    body.0.denoiser          |    0.7M              |\n",
      "Total Parameters: 0.75 M\n",
      "FLOPs calculation failed: forward() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "from basicsr.models.archs.RetinexFormer_arch import RetinexFormer\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = RetinexFormer(\n",
    "    in_channels=3, out_channels=3, n_feat=32, stage=1, num_blocks=[1, 1, 1]\n",
    ").cuda()\n",
    "model.eval()\n",
    "\n",
    "# Prepare dummy input: tuple (RGB tensor, Thermal tensor)\n",
    "# Assuming 256x256 resolution, batch size 1\n",
    "rgb_tensor = torch.randn(1, 3, 256, 256).cuda()\n",
    "thermal_tensor = torch.randn(1, 3, 256, 256).cuda()\n",
    "\n",
    "# Wrap as tuple, as expected by forward()\n",
    "inputs = (rgb_tensor, thermal_tensor)\n",
    "\n",
    "# Count parameters\n",
    "param_table = parameter_count_table(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nParameter Count Table:\\n{param_table}\")\n",
    "print(f\"Total Parameters: {total_params / 1e6:.2f} M\")\n",
    "\n",
    "# Count FLOPs (you need to pass inputs as tuple)\n",
    "try:\n",
    "    flops = FlopCountAnalysis(model, inputs)\n",
    "    print(f\"Total FLOPs: {flops.total() / 1e9:.2f} GFLOPs\")\n",
    "except Exception as e:\n",
    "    print(f\"FLOPs calculation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "dataset LLVIP\n",
      "Not using Automatic Mixed Precision\n",
      "===>Testing using weights:  /mnt/data/Raman/Retinexformer/pretrained_weights/LLVIP_best.pth\n",
      "data/LLVIP/test/input\n",
      "data/LLVIP/test/thermal\n",
      "data/LLVIP/test/target\n",
      "  3%|█▎                                          | 2/70 [00:12<07:06,  6.28s/it]^C\n",
      "  3%|█▎                                          | 2/70 [00:13<07:52,  6.95s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"Enhancement/test_from_dataset.py\", line 194, in <module>\n",
      "    restored = model_restoration(input_tuple)\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 166, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/mnt/data/Raman/Retinexformer/basicsr/models/archs/RTxNet_arch.py\", line 431, in forward\n",
      "    out = self.body(x)\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/mnt/data/Raman/Retinexformer/basicsr/models/archs/RTxNet_arch.py\", line 404, in forward\n",
      "    reduced_features_np = pca.fit_transform(flattened_features.detach().cpu().numpy())\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/sklearn/decomposition/_pca.py\", line 407, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/sklearn/decomposition/_pca.py\", line 459, in _fit\n",
      "    return self._fit_truncated(X, n_components, self._fit_svd_solver)\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/sklearn/decomposition/_pca.py\", line 585, in _fit_truncated\n",
      "    random_state=random_state,\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/sklearn/utils/extmath.py\", line 400, in randomized_svd\n",
      "    random_state=random_state,\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/sklearn/utils/extmath.py\", line 237, in randomized_range_finder\n",
      "    Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)\n",
      "  File \"/home/raman/ENTER/envs/Retinexformer/lib/python3.7/site-packages/scipy/linalg/decomp_lu.py\", line 216, in lu\n",
      "    p, l, u, info = flu(a1, permute_l=permute_l, overwrite_a=overwrite_a)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 Enhancement/test_from_dataset.py --opt Options/RTxNet_LLVIP.yml --weights /mnt/data/Raman/Retinexformer/pretrained_weights/LLVIP_best.pth --dataset LLVIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Retinexformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
